# AgentsAI - Complete Voice Agent Development Journey

## üéØ Project Overview

**AgentsAI** is a sophisticated AI-powered voice agent application built incrementally over 14 days, demonstrating the evolution from basic text-to-speech to advanced conversational AI with modern architecture.

### üöÄ **Current Status**
- **Progress**: Day 14 of 30 completed (47% complete)
- **Architecture**: Production-ready with clean, modular design
- **Features**: Complete voice conversation pipeline with memory and error handling
- **UI/UX**: Modern glassmorphism interface with accessibility features

### üèóÔ∏è **Tech Stack**
- **Backend**: Python Flask + Pydantic + Professional logging
- **AI Services**: Google Gemini AI (LLM) + Murf API (TTS) + AssemblyAI (STT)
- **Frontend**: HTML5 + CSS3 + JavaScript + MediaRecorder API
- **Architecture**: Service-oriented design with comprehensive error handling

---

## üìÅ Project Structure

```
AgentsAI/
‚îú‚îÄ‚îÄ üìÅ config/                    # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              
‚îÇ   ‚îî‚îÄ‚îÄ settings.py              # Environment validation & logging setup
‚îú‚îÄ‚îÄ üìÅ models/                   # Pydantic data models
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py               # Request/response validation schemas
‚îú‚îÄ‚îÄ üìÅ services/                 # Third-party service integrations
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ stt.py                  # Speech-to-Text (AssemblyAI)
‚îÇ   ‚îú‚îÄ‚îÄ llm.py                  # Large Language Model (Gemini)
‚îÇ   ‚îî‚îÄ‚îÄ tts.py                  # Text-to-Speech (Murf)
‚îú‚îÄ‚îÄ üìÅ utils/                    # Utility functions
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ chat_history.py         # Session management & memory
‚îÇ   ‚îî‚îÄ‚îÄ error_handling.py       # Error utilities & validation
‚îú‚îÄ‚îÄ üìÅ static/                   # Frontend assets
‚îÇ   ‚îú‚îÄ‚îÄ css/style.css           # Modern glassmorphism design
‚îÇ   ‚îî‚îÄ‚îÄ js/voice-button.js      # Single toggle voice interface
‚îú‚îÄ‚îÄ üìÅ templates/                # HTML templates
‚îÇ   ‚îî‚îÄ‚îÄ index.html              # Modern responsive UI
‚îú‚îÄ‚îÄ üìÅ uploads/                  # Audio file storage
‚îú‚îÄ‚îÄ app.py                       # Main Flask application (refactored)
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ .env                         # Environment variables (API keys)
‚îî‚îÄ‚îÄ README.md                    # This documentation
```

---

## üõ†Ô∏è Installation & Setup

### 1. **Clone Repository**
```bash
git clone <repository-url>
cd AgentsAI
```

### 2. **Python Environment Setup**
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 3. **Environment Configuration**
Create `.env` file with your API keys:
```bash
# AI Service API Keys
ASSEMBLYAI_API_KEY=your_assemblyai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
MURF_API_KEY=your_murf_api_key_here
MURF_API_URL=your_murf_api_url_here

# Application Configuration
DEBUG=True
HOST=localhost
PORT=5000
LOG_LEVEL=INFO
```

### 4. **Run Application**
```bash
python app.py
```

Visit `http://localhost:5000` to access the voice agent interface.

---

## üéØ Core Features

### üéôÔ∏è **Voice Processing Pipeline**
- **Speech-to-Text**: High-accuracy transcription using AssemblyAI
- **LLM Processing**: Intelligent responses via Google Gemini AI
- **Text-to-Speech**: Natural voice synthesis with Murf API
- **Real-time Processing**: Live audio capture and processing feedback

### üß† **Conversational Intelligence**
- **Memory System**: Session-based chat history with context awareness
- **Context Management**: Maintains conversation flow across interactions
- **Intelligent Responses**: Context-aware AI responses with personality
- **Multi-turn Conversations**: Supports extended dialogues

### üõ°Ô∏è **Production-Ready Features**
- **Three-Layer Error Handling**: Server ‚Üí Emergency ‚Üí Client fallbacks
- **Health Monitoring**: Service status endpoints for monitoring
- **Professional Logging**: Structured logging with proper levels
- **Type Safety**: Pydantic models for all API contracts
- **Validation**: Automatic input validation and error prevention

### üé® **Modern User Interface**
- **Glassmorphism Design**: Frosted glass aesthetic with backdrop blur
- **Single Voice Button**: 200px animated toggle with state management
- **Responsive Design**: Mobile-first approach with touch optimization
- **Accessibility**: Keyboard shortcuts and ARIA labels
- **Visual Feedback**: Real-time status updates and animations

---

## üìä Development Journey (Days 1-14)

---

## üîµ **DAY 1: Project Setup & Basic TTS**
**Goal**: Create basic Flask application with text-to-speech functionality using Murf API.

**üìã What I Did on Day 1:**
- Set up initial Flask project structure and environment
- Integrated Murf API for text-to-speech conversion
- Created basic HTML interface for text input and audio playback
- Configured environment variables for API keys
- Implemented basic error handling for TTS requests
- Tested text-to-speech conversion with various voice settings

**Implementation**:
```python
# Day 1: Basic Flask TTS Application
from flask import Flask, render_template, request, jsonify
import requests
import os
from dotenv import load_dotenv

load_dotenv()
app = Flask(__name__)

# Environment Configuration
MURF_API_KEY = os.getenv('MURF_API_KEY')
MURF_API_URL = os.getenv('MURF_API_URL')

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/tts', methods=['POST'])
def text_to_speech():
    """Convert text to speech using Murf API"""
    try:
        data = request.get_json()
        text = data.get('text', '')
        
        murf_payload = {
            "text": text,
            "voice_id": "en-US-AriaNeural", 
            "speed": 95,
            "pitch": 45
        }
        
        headers = {
            'Authorization': f'Bearer {MURF_API_KEY}',
            'Content-Type': 'application/json'
        }
        
        response = requests.post(MURF_API_URL, json=murf_payload, headers=headers)
        return jsonify(response.json())
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True, host='localhost', port=5000)
```

**Frontend (Day 1)**:
```html
<!-- Basic HTML interface -->
<div class="container">
    <h1>AI Voice Agent - Day 1</h1>
    <textarea id="textInput" placeholder="Enter text to convert to speech..."></textarea>
    <button id="generateBtn">Generate Speech</button>
    <audio id="audioResult" controls style="display:none;"></audio>
</div>
```

- ‚úÖ **Clean Architecture**: Modular, maintainable codebase
- ‚úÖ **Type Safety**: Pydantic models with validation
- ‚úÖ **Error Resilience**: Three-layer fallback system
- ‚úÖ **Production Ready**: Logging, monitoring, health checks
- ‚úÖ **Test Coverage**: Comprehensive automated testing

---

## üîµ **DAY 2-3: Audio File Handling & Upload System**
**Goal**: Implement secure file upload system for audio processing and storage.

**üìã What I Did on Day 2-3:**
- Implemented secure file upload functionality with validation
- Added support for multiple audio formats (WAV, MP3, OGG, WebM, M4A)
- Created drag & drop interface for intuitive file uploads
- Implemented file size limitations and security checks
- Built temporary file management system for audio processing
- Added unique filename generation to prevent conflicts
- Enhanced UI with upload progress indicators and error messages

**Implementation**:
```python
# Day 2-3: File Upload and Audio Handling
from werkzeug.utils import secure_filename
import tempfile
import uuid

# Configuration for file uploads
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

ALLOWED_EXTENSIONS = {'wav', 'mp3', 'ogg', 'webm', 'm4a'}

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/api/upload', methods=['POST'])
def upload_audio():
    """Handle audio file uploads with security validation"""
    try:
        if 'audio' not in request.files:
            return jsonify({'error': 'No audio file provided'}), 400
        
        file = request.files['audio']
        
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400
        
        if file and allowed_file(file.filename):
            # Generate unique filename to prevent conflicts
            file_extension = file.filename.rsplit('.', 1)[1].lower()
            unique_filename = f"{uuid.uuid4()}.{file_extension}"
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], unique_filename)
            
            # Ensure upload directory exists
            os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
            
            # Save file securely
            file.save(filepath)
            
            return jsonify({
                'success': True, 
                'filename': unique_filename,
                'filepath': filepath,
                'size': os.path.getsize(filepath)
            })
        
        return jsonify({'error': 'Invalid file type'}), 400
        
    except Exception as e:
        return jsonify({'error': f'Upload failed: {str(e)}'}), 500

def save_temp_file(audio_file):
    """Save uploaded file to temporary location for processing"""
    temp_dir = tempfile.gettempdir()
    temp_filename = f"audio_{uuid.uuid4()}.{audio_file.filename.split('.')[-1]}"
    temp_path = os.path.join(temp_dir, temp_filename)
    audio_file.save(temp_path)
    return temp_path

def cleanup_temp_file(filepath):
    """Clean up temporary files after processing"""
    try:
        if os.path.exists(filepath):
            os.remove(filepath)
    except Exception as e:
        print(f"Warning: Could not clean up temp file {filepath}: {e}")
```

**Frontend Enhancement (Day 2-3)**:
```javascript
// Audio file upload with drag & drop
class AudioUploader {
    constructor() {
        this.setupDropZone();
        this.setupFileInput();
    }
    
    setupDropZone() {
        const dropZone = document.getElementById('dropZone');
        
        dropZone.addEventListener('dragover', (e) => {
            e.preventDefault();
            dropZone.classList.add('dragover');
        });
        
        dropZone.addEventListener('drop', (e) => {
            e.preventDefault();
            dropZone.classList.remove('dragover');
            this.handleFiles(e.dataTransfer.files);
        });
    }
    
    async handleFiles(files) {
        const file = files[0];
        if (!this.validateAudioFile(file)) return;
        
        const formData = new FormData();
        formData.append('audio', file);
        
        try {
            const response = await fetch('/api/upload', {
                method: 'POST',
                body: formData
            });
            
            const result = await response.json();
            if (result.success) {
                this.displayUploadSuccess(result);
            }
        } catch (error) {
            this.displayError('Upload failed: ' + error.message);
        }
    }
}
```

- ‚úÖ Support for multiple audio formats (WAV, MP3, OGG, WebM, M4A)
- ‚úÖ Drag & drop interface
- ‚úÖ File size limitations and security checks
- ‚úÖ Temporary file management
- ‚úÖ Unique filename generation to prevent conflicts

---

## üîµ **DAY 4-5: Speech-to-Text Integration**
**Goal**: Integrate AssemblyAI for high-accuracy audio transcription with comprehensive error handling.

**üìã What I Did on Day 4-5:**
- Integrated AssemblyAI API for speech-to-text conversion
- Implemented automatic language detection and speaker labeling
- Added word-level timestamps and confidence scoring
- Built comprehensive error handling with retry logic
- Created detailed transcription metadata (duration, word count)
- Implemented exponential backoff for failed requests
- Added real-time transcription progress feedback in UI
- Optimized audio preprocessing for better accuracy

**Implementation**:
```python
# Day 4-5: AssemblyAI Speech-to-Text Integration
import assemblyai as aai
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# AssemblyAI Configuration
aai.settings.api_key = os.getenv('ASSEMBLYAI_API_KEY')

class TranscriptionService:
    def __init__(self):
        self.api_key = os.getenv('ASSEMBLYAI_API_KEY')
        self.validate_api_key()
    
    def validate_api_key(self):
        if not self.api_key:
            logger.error("AssemblyAI API key not found in environment variables")
            raise ValueError("AssemblyAI API key is required")
    
    def transcribe_audio_file(self, audio_file_path):
        """Transcribe audio file using AssemblyAI with detailed response"""
        try:
            transcriber = aai.Transcriber()
            
            # Configure transcription settings
            config = aai.TranscriptionConfig(
                speech_model=aai.SpeechModel.best,
                language_detection=True,
                speaker_labels=True
            )
            
            logger.info(f"Starting transcription for: {audio_file_path}")
            transcript = transcriber.transcribe(audio_file_path, config=config)
            
            if transcript.status == aai.TranscriptStatus.error:
                logger.error(f"Transcription failed: {transcript.error}")
                return None
            
            result = {
                'text': transcript.text,
                'confidence': transcript.confidence,
                'language_detected': getattr(transcript, 'language_detected', None),
                'audio_duration': getattr(transcript, 'audio_duration', None),
                'words': [
                    {
                        'text': word.text,
                        'start': word.start,
                        'end': word.end,
                        'confidence': word.confidence
                    } for word in (transcript.words or [])
                ]
            }
            
            logger.info(f"Transcription completed successfully")
            return result
            
        except Exception as e:
            logger.error(f"Transcription error: {str(e)}")
            return None

@app.route('/api/transcribe', methods=['POST'])
def transcribe_audio():
    """API endpoint for audio transcription"""
    try:
        if 'audio' not in request.files:
            return jsonify({'error': 'No audio file provided'}), 400
        
        audio_file = request.files['audio']
        
        # Save to temporary file for processing
        temp_path = save_temp_file(audio_file)
        
        try:
            # Initialize transcription service
            transcription_service = TranscriptionService()
            
            # Perform transcription
            result = transcription_service.transcribe_audio_file(temp_path)
            
            if result:
                response_data = {
                    'success': True,
                    'transcription': result['text'],
                    'confidence': result['confidence'],
                    'details': {
                        'language_detected': result.get('language_detected'),
                        'audio_duration': result.get('audio_duration'),
                        'word_count': len(result.get('words', [])),
                        'timestamp': datetime.now().isoformat()
                    }
                }
                
                # Optional: Include detailed word-level timestamps
                if request.args.get('include_words') == 'true':
                    response_data['words'] = result['words']
                
                return jsonify(response_data)
            else:
                return jsonify({
                    'success': False,
                    'error': 'Transcription failed'
                }), 500
        
        finally:
            # Always clean up temporary file
            cleanup_temp_file(temp_path)
    
    except Exception as e:
        logger.error(f"Transcription endpoint error: {str(e)}")
        return jsonify({
            'success': False,
            'error': 'Internal server error'
        }), 500

# Enhanced error handling for transcription
def handle_transcription_with_retry(audio_file_path, max_retries=3):
    """Transcribe with automatic retry logic"""
    for attempt in range(max_retries):
        try:
            service = TranscriptionService()
            result = service.transcribe_audio_file(audio_file_path)
            if result:
                return result
        except Exception as e:
            logger.warning(f"Transcription attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                raise e
            time.sleep(2 ** attempt)  # Exponential backoff
    
    return None
```

**Frontend Integration (Day 4-5)**:
```javascript
// Audio transcription with real-time feedback
class AudioTranscriber {
    constructor() {
        this.isTranscribing = false;
    }
    
    async transcribeAudio(audioBlob) {
        if (this.isTranscribing) return;
        
        this.isTranscribing = true;
        this.showTranscriptionProgress();
        
        try {
            const formData = new FormData();
            formData.append('audio', audioBlob, 'recording.wav');
            
            const response = await fetch('/api/transcribe', {
                method: 'POST',
                body: formData
            });
            
            const result = await response.json();
            
            if (result.success) {
                this.displayTranscriptionResult(result);
            } else {
                this.displayError(result.error);
            }
            
        } catch (error) {
            this.displayError('Transcription failed: ' + error.message);
        } finally {
            this.isTranscribing = false;
            this.hideTranscriptionProgress();
        }
    }
    
    displayTranscriptionResult(result) {
        const transcriptionDiv = document.getElementById('transcriptionResult');
        transcriptionDiv.innerHTML = `
            <div class="transcription-success">
                <h3>Transcription Result:</h3>
                <p class="transcribed-text">${result.transcription}</p>
                <div class="transcription-details">
                    <span>Confidence: ${(result.confidence * 100).toFixed(1)}%</span>
                    <span>Language: ${result.details.language_detected || 'Auto-detected'}</span>
                    <span>Duration: ${result.details.audio_duration || 'N/A'}s</span>
                </div>
            </div>
        `;
    }
}
```

- ‚úÖ Comprehensive error handling with retry logic
- ‚úÖ Real-time transcription progress feedback
- ‚úÖ Detailed transcription metadata (duration, word count, etc.)

---

## üîµ **DAY 6-7: Echo Bot Development**
**Goal**: Create a complete voice echo system that records, transcribes, and regenerates speech.

**üìã What I Did on Day 6-7:**
- Built complete voice echo pipeline (Audio ‚Üí Text ‚Üí Audio)
- Implemented real-time audio recording using MediaRecorder API
- Created session-based echo history tracking system
- Added processing time measurement and optimization
- Built comprehensive error handling for the audio pipeline
- Implemented modern UI with recording state management
- Added audio playback controls for generated speech
- Optimized audio quality and voice synthesis parameters

**Implementation**:
```python
# Day 6-7: Voice Echo Bot - Complete Audio Pipeline
from datetime import datetime
import json

class VoiceEchoBot:
    def __init__(self):
        self.transcription_service = TranscriptionService()
        self.session_data = {}
    
    def process_voice_echo(self, audio_file, session_id=None):
        """Complete voice echo pipeline: Audio ‚Üí Text ‚Üí Audio"""
        if not session_id:
            session_id = str(uuid.uuid4())
        
        try:
            # Step 1: Save and validate audio file
            temp_path = save_temp_file(audio_file)
            
            # Step 2: Transcribe audio to text
            transcription_result = self.transcription_service.transcribe_audio_file(temp_path)
            
            if not transcription_result:
                return {'error': 'Transcription failed'}, 500
            
            original_text = transcription_result['text']
            
            # Step 3: Generate TTS from transcription
            tts_result = self.generate_echo_tts(original_text)
            
            if not tts_result:
                return {'error': 'TTS generation failed'}, 500
            
            # Step 4: Store session data
            self.store_echo_session(session_id, {
                'original_text': original_text,
                'confidence': transcription_result['confidence'],
                'audio_url': tts_result['audio_url'],
                'timestamp': datetime.now().isoformat()
            })
            
            return {
                'success': True,
                'session_id': session_id,
                'original_text': original_text,
                'confidence': transcription_result['confidence'],
                'audio_url': tts_result['audio_url'],
                'processing_time': tts_result.get('processing_time', 0)
            }
            
        finally:
            cleanup_temp_file(temp_path)
    
    def generate_echo_tts(self, text):
        """Generate TTS audio from transcribed text"""
        try:
            murf_payload = {
                "text": text,
                "voice_id": "en-US-AriaNeural",
                "speed": 95,
                "pitch": 45,
                "format": "mp3"
            }
            
            headers = {
                'Authorization': f'Bearer {MURF_API_KEY}',
                'Content-Type': 'application/json'
            }
            
            start_time = datetime.now()
            response = requests.post(MURF_API_URL, json=murf_payload, headers=headers)
            processing_time = (datetime.now() - start_time).total_seconds()
            
            if response.status_code == 200:
                result = response.json()
                return {
                    'audio_url': result.get('audio_url'),
                    'processing_time': processing_time
                }
            else:
                logger.error(f"TTS generation failed: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"TTS generation error: {str(e)}")
            return None
    
    def store_echo_session(self, session_id, data):
        """Store echo session data for history tracking"""
        self.session_data[session_id] = data

# Echo Bot API Endpoints
echo_bot = VoiceEchoBot()

@app.route('/api/echo', methods=['POST'])
def voice_echo():
    """Voice echo endpoint: Record ‚Üí Transcribe ‚Üí Regenerate"""
    try:
        if 'audio' not in request.files:
            return jsonify({'error': 'No audio file provided'}), 400
        
        audio_file = request.files['audio']
        session_id = request.form.get('session_id')
        
        result = echo_bot.process_voice_echo(audio_file, session_id)
        
        if 'error' in result:
            return jsonify(result), 500
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"Echo endpoint error: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500

@app.route('/api/echo/history/<session_id>')
def echo_history(session_id):
    """Get echo session history"""
    if session_id in echo_bot.session_data:
        return jsonify({
            'session_id': session_id,
            'data': echo_bot.session_data[session_id]
        })
    else:
        return jsonify({'error': 'Session not found'}), 404
```

**Enhanced Frontend (Day 6-7)**:
```javascript
// Complete Voice Echo Interface
class VoiceEchoInterface {
    constructor() {
        this.mediaRecorder = null;
        this.audioChunks = [];
        this.isRecording = false;
        this.sessionId = this.generateSessionId();
    }
    
    async startRecording() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            this.mediaRecorder = new MediaRecorder(stream);
            this.audioChunks = [];
            
            this.mediaRecorder.ondataavailable = (event) => {
                this.audioChunks.push(event.data);
            };
            
            this.mediaRecorder.onstop = () => {
                this.processRecording();
            };
            
            this.mediaRecorder.start();
            this.isRecording = true;
            this.updateUIRecording();
            
        } catch (error) {
            this.displayError('Microphone access denied: ' + error.message);
        }
    }
    
    stopRecording() {
        if (this.mediaRecorder && this.isRecording) {
            this.mediaRecorder.stop();
            this.isRecording = false;
            this.updateUIProcessing();
        }
    }
    
    async processRecording() {
        const audioBlob = new Blob(this.audioChunks, { type: 'audio/wav' });
        
        const formData = new FormData();
        formData.append('audio', audioBlob, 'echo_recording.wav');
        formData.append('session_id', this.sessionId);
        
        try {
            const response = await fetch('/api/echo', {
                method: 'POST',
                body: formData
            });
            
            const result = await response.json();
            
            if (result.success) {
                this.displayEchoResult(result);
            } else {
                this.displayError(result.error);
            }
            
        } catch (error) {
            this.displayError('Echo processing failed: ' + error.message);
        } finally {
            this.updateUIIdle();
        }
    }
    
    displayEchoResult(result) {
        const resultsDiv = document.getElementById('echoResults');
        resultsDiv.innerHTML = `
            <div class="echo-result">
                <h3>Echo Result</h3>
                <div class="transcription">
                    <strong>You said:</strong> "${result.original_text}"
                </div>
                <div class="confidence">
                    Confidence: ${(result.confidence * 100).toFixed(1)}%
                </div>
                <div class="audio-playback">
                    <audio controls>
                        <source src="${result.audio_url}" type="audio/mpeg">
                        Your browser does not support audio playback.
                    </audio>
                </div>
                <div class="processing-time">
                    Processing time: ${result.processing_time.toFixed(2)}s
                </div>
            </div>
        `;
    }
}

// Initialize Echo Bot Interface
const echoBot = new VoiceEchoInterface();
```

- ‚úÖ Modern UI with recording state management
- ‚úÖ Audio playback controls for generated speech

---

## üîµ **DAY 8: LLM Integration**
**Goal**: Integrate Google Gemini AI for intelligent text processing and response generation.

**üìã What I Did on Day 8:**
- Integrated Google Gemini AI for intelligent text processing
- Configured generation settings for consistent, conversational responses
- Implemented safety filtering and content moderation
- Added token usage tracking and optimization
- Built conversational prompt engineering for voice synthesis
- Created health monitoring for LLM service
- Added response time measurement and performance tracking
- Optimized prompts specifically for voice delivery (concise, natural)

**Implementation**:
```python
# Day 8: Google Gemini AI Integration for Intelligent Responses
import google.generativeai as genai
import time
from typing import Optional, Dict, Any

class LLMService:
    def __init__(self):
        self.api_key = os.getenv('GEMINI_API_KEY')
        self.model_name = 'gemini-1.5-flash'
        self.setup_client()
    
    def setup_client(self):
        """Initialize Gemini AI client with configuration"""
        if not self.api_key:
            raise ValueError("Gemini API key is required")
        
        try:
            genai.configure(api_key=self.api_key)
            
            # Configure generation settings for consistent responses
            self.generation_config = genai.types.GenerationConfig(
                candidate_count=1,
                max_output_tokens=1000,
                temperature=0.7,
                top_p=0.8,
                top_k=40
            )
            
            # Safety settings to prevent harmful content
            self.safety_settings = [
                {
                    "category": "HARM_CATEGORY_HARASSMENT",
                    "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                    "category": "HARM_CATEGORY_HATE_SPEECH", 
                    "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                }
            ]
            
            self.model = genai.GenerativeModel(
                model_name=self.model_name,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings
            )
            
            logger.info(f"Gemini AI model '{self.model_name}' initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize Gemini AI: {str(e)}")
            raise
    
    def generate_response(self, prompt: str, context: Optional[str] = None) -> Dict[str, Any]:
        """Generate intelligent response using Gemini AI"""
        try:
            # Prepare enhanced prompt with context
            enhanced_prompt = self.prepare_prompt(prompt, context)
            
            start_time = time.time()
            
            # Generate response
            response = self.model.generate_content(enhanced_prompt)
            
            processing_time = time.time() - start_time
            
            # Handle potential safety blocks
            if response.prompt_feedback.block_reason:
                return {
                    'success': False,
                    'error': 'Content blocked due to safety concerns',
                    'block_reason': str(response.prompt_feedback.block_reason)
                }
            
            # Extract response text
            if response.candidates and response.candidates[0].content.parts:
                response_text = response.candidates[0].content.parts[0].text
                
                return {
                    'success': True,
                    'response': response_text,
                    'model': self.model_name,
                    'processing_time': processing_time,
                    'usage_metadata': {
                        'prompt_token_count': getattr(response, 'usage_metadata', {}).get('prompt_token_count', 0),
                        'candidates_token_count': getattr(response, 'usage_metadata', {}).get('candidates_token_count', 0),
                        'total_token_count': getattr(response, 'usage_metadata', {}).get('total_token_count', 0)
                    }
                }
            else:
                return {
                    'success': False,
                    'error': 'No response generated'
                }
                
        except Exception as e:
            logger.error(f"LLM generation error: {str(e)}")
            return {
                'success': False,
                'error': f'Response generation failed: {str(e)}'
            }
    
    def prepare_prompt(self, user_input: str, context: Optional[str] = None) -> str:
        """Prepare enhanced prompt with system instructions and context"""
        system_prompt = """You are a helpful, friendly AI assistant specializing in voice interactions. 
        Your responses should be:
        - Conversational and natural for speech synthesis
        - Concise but informative (aim for 1-3 sentences)
        - Engaging and helpful
        - Appropriate for voice delivery
        
        Avoid:
        - Very long responses that are tedious when spoken
        - Technical jargon unless specifically asked
        - Lists or bullet points (use conversational alternatives)
        """
        
        if context:
            enhanced_prompt = f"{system_prompt}\n\nContext: {context}\n\nUser: {user_input}\n\nAssistant:"
        else:
            enhanced_prompt = f"{system_prompt}\n\nUser: {user_input}\n\nAssistant:"
        
        return enhanced_prompt

# Initialize LLM service
llm_service = LLMService()

@app.route('/llm/query', methods=['POST'])
def llm_query():
    """LLM text processing endpoint"""
    try:
        data = request.get_json()
        
        if not data or 'text' not in data:
            return jsonify({'error': 'No text provided'}), 400
        
        user_text = data['text'].strip()
        context = data.get('context', '')
        
        if not user_text:
            return jsonify({'error': 'Empty text provided'}), 400
        
        # Generate AI response
        result = llm_service.generate_response(user_text, context)
        
        if result['success']:
            response_data = {
                'success': True,
                'query': user_text,
                'response': result['response'],
                'model': result['model'],
                'processing_time': result['processing_time'],
                'usage_metadata': result['usage_metadata'],
                'timestamp': datetime.now().isoformat()
            }
            
            return jsonify(response_data)
        else:
            return jsonify({
                'success': False,
                'error': result['error'],
                'query': user_text
            }), 500
            
    except Exception as e:
        logger.error(f"LLM query endpoint error: {str(e)}")
        return jsonify({
            'success': False,
            'error': 'Internal server error'
        }), 500

@app.route('/llm/health')
def llm_health():
    """Check LLM service health"""
    try:
        test_response = llm_service.generate_response("Hello")
        if test_response['success']:
            return jsonify({
                'status': 'healthy',
                'model': llm_service.model_name,
                'test_response_time': test_response['processing_time']
            })
        else:
            return jsonify({
                'status': 'unhealthy',
                'error': test_response['error']
            }), 503
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e)
        }), 503
```

**Frontend Integration (Day 8)**:
```javascript
// LLM Text Processing Interface
class LLMInterface {
    constructor() {
        this.isProcessing = false;
        this.conversationHistory = [];
    }
    
    async processText(userInput, includeContext = true) {
        if (this.isProcessing) return;
        
        this.isProcessing = true;
        this.showProcessingState();
        
        try {
            const payload = {
                text: userInput,
                context: includeContext ? this.getRecentContext() : ''
            };
            
            const response = await fetch('/llm/query', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify(payload)
            });
            
            const result = await response.json();
            
            if (result.success) {
                this.addToHistory('user', userInput);
                this.addToHistory('assistant', result.response);
                this.displayLLMResponse(result);
            } else {
                this.displayError(result.error);
            }
            
        } catch (error) {
            this.displayError('Failed to process text: ' + error.message);
        } finally {
            this.isProcessing = false;
            this.hideProcessingState();
        }
    }
    
    addToHistory(role, content) {
        this.conversationHistory.push({
            role: role,
            content: content,
            timestamp: new Date().toISOString()
        });
        
        // Keep only last 10 exchanges for context
        if (this.conversationHistory.length > 20) {
            this.conversationHistory = this.conversationHistory.slice(-20);
        }
    }
    
    getRecentContext() {
        return this.conversationHistory
            .slice(-6) // Last 3 exchanges
            .map(msg => `${msg.role}: ${msg.content}`)
            .join('\n');
    }
    
    displayLLMResponse(result) {
        const responseDiv = document.getElementById('llmResponse');
        responseDiv.innerHTML = `
            <div class="llm-response">
                <div class="user-query">
                    <strong>You:</strong> ${result.query}
                </div>
                <div class="ai-response">
                    <strong>AI:</strong> ${result.response}
                </div>
                <div class="response-metadata">
                    <span>Model: ${result.model}</span>
                    <span>Time: ${result.processing_time.toFixed(2)}s</span>
                    <span>Tokens: ${result.usage_metadata.total_token_count}</span>
                </div>
            </div>
        `;
    }
}

// Initialize LLM Interface
const llmInterface = new LLMInterface();
```

- ‚úÖ Health monitoring for LLM service
- ‚úÖ Response time measurement and performance tracking

---

## üîµ **DAY 9: Complete Voice Assistant**
**Goal**: Combine STT + LLM + TTS into a complete conversational AI voice assistant.

**üìã What I Did on Day 9:**
- Combined all services into complete voice-to-voice conversation pipeline
- Implemented session-based conversation memory and context awareness
- Added detailed performance monitoring with timing for each stage
- Built text optimization for natural speech synthesis
- Created robust error handling across all pipeline stages
- Implemented conversation history tracking and retrieval
- Added real-time processing status updates in UI
- Optimized the entire pipeline for speed and reliability

**Implementation**:
```python
# Day 9: Complete Voice Assistant Pipeline
from dataclasses import dataclass
from typing import Optional, Dict, List
import json

@dataclass
class VoiceProcessingResult:
    success: bool
    transcription: Optional[str] = None
    llm_response: Optional[str] = None
    audio_url: Optional[str] = None
    error: Optional[str] = None
    processing_times: Optional[Dict[str, float]] = None
    session_id: Optional[str] = None

class VoiceAssistant:
    def __init__(self):
        self.transcription_service = TranscriptionService()
        self.llm_service = LLMService()
        self.conversation_sessions = {}
    
    def process_voice_query(self, audio_file, session_id: Optional[str] = None) -> VoiceProcessingResult:
        """Complete voice processing pipeline: Audio ‚Üí Text ‚Üí AI ‚Üí Audio"""
        
        if not session_id:
            session_id = str(uuid.uuid4())
        
        processing_times = {}
        temp_path = None
        
        try:
            # Step 1: Save audio file temporarily
            temp_path = save_temp_file(audio_file)
            
            # Step 2: Speech-to-Text
            start_time = time.time()
            transcription_result = self.transcription_service.transcribe_audio_file(temp_path)
            processing_times['stt'] = time.time() - start_time
            
            if not transcription_result:
                return VoiceProcessingResult(
                    success=False,
                    error="Speech transcription failed",
                    session_id=session_id
                )
            
            user_text = transcription_result['text']
            logger.info(f"Transcribed: {user_text}")
            
            # Step 3: Get conversation context
            context = self.get_conversation_context(session_id)
            
            # Step 4: LLM Processing
            start_time = time.time()
            llm_result = self.llm_service.generate_response(user_text, context)
            processing_times['llm'] = time.time() - start_time
            
            if not llm_result['success']:
                return VoiceProcessingResult(
                    success=False,
                    transcription=user_text,
                    error=f"AI processing failed: {llm_result['error']}",
                    session_id=session_id
                )
            
            ai_response = llm_result['response']
            logger.info(f"AI Response: {ai_response}")
            
            # Step 5: Text-to-Speech
            start_time = time.time()
            tts_result = self.generate_response_audio(ai_response)
            processing_times['tts'] = time.time() - start_time
            
            if not tts_result:
                return VoiceProcessingResult(
                    success=False,
                    transcription=user_text,
                    llm_response=ai_response,
                    error="Speech synthesis failed",
                    session_id=session_id
                )
            
            # Step 6: Update conversation history
            self.update_conversation_history(session_id, user_text, ai_response)
            
            return VoiceProcessingResult(
                success=True,
                transcription=user_text,
                llm_response=ai_response,
                audio_url=tts_result['audio_url'],
                processing_times=processing_times,
                session_id=session_id
            )
            
        except Exception as e:
            logger.error(f"Voice processing error: {str(e)}")
            return VoiceProcessingResult(
                success=False,
                error=f"Processing failed: {str(e)}",
                session_id=session_id
            )
        
        finally:
            if temp_path:
                cleanup_temp_file(temp_path)
    
    def generate_response_audio(self, text: str) -> Optional[Dict]:
        """Generate TTS audio for AI response"""
        try:
            # Optimize text for speech synthesis
            speech_text = self.optimize_for_speech(text)
            
            murf_payload = {
                "text": speech_text,
                "voice_id": "en-US-AriaNeural",
                "speed": 90,  # Slightly slower for AI responses
                "pitch": 50,
                "format": "mp3"
            }
            
            headers = {
                'Authorization': f'Bearer {MURF_API_KEY}',
                'Content-Type': 'application/json'
            }
            
            response = requests.post(MURF_API_URL, json=murf_payload, headers=headers)
            
            if response.status_code == 200:
                return response.json()
            else:
                logger.error(f"TTS generation failed: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"TTS generation error: {str(e)}")
            return None
    
    def optimize_for_speech(self, text: str) -> str:
        """Optimize text for natural speech synthesis"""
        # Add pauses for better speech flow
        text = text.replace('. ', '. ')  # Ensure space after periods
        text = text.replace('! ', '! ')  # Ensure space after exclamations
        text = text.replace('? ', '? ')  # Ensure space after questions
        
        # Handle common abbreviations
        replacements = {
            'e.g.': 'for example',
            'i.e.': 'that is',
            'etc.': 'and so on',
            'vs.': 'versus',
            'AI': 'A I',
            'API': 'A P I',
            'URL': 'U R L'
        }
        
        for abbrev, replacement in replacements.items():
            text = text.replace(abbrev, replacement)
        
        return text
    
    def get_conversation_context(self, session_id: str) -> str:
        """Get recent conversation context for the session"""
        if session_id not in self.conversation_sessions:
            return ""
        
        history = self.conversation_sessions[session_id]
        # Return last 3 exchanges as context
        recent_context = history[-6:] if len(history) > 6 else history
        
        context_text = []
        for entry in recent_context:
            context_text.append(f"{entry['role']}: {entry['content']}")
        
        return '\n'.join(context_text)
    
    def update_conversation_history(self, session_id: str, user_input: str, ai_response: str):
        """Update conversation history for the session"""
        if session_id not in self.conversation_sessions:
            self.conversation_sessions[session_id] = []
        
        timestamp = datetime.now().isoformat()
        
        self.conversation_sessions[session_id].extend([
            {
                'role': 'user',
                'content': user_input,
                'timestamp': timestamp
            },
            {
                'role': 'assistant', 
                'content': ai_response,
                'timestamp': timestamp
            }
        ])
        
        # Keep only last 50 messages per session
        if len(self.conversation_sessions[session_id]) > 50:
            self.conversation_sessions[session_id] = self.conversation_sessions[session_id][-50:]

# Initialize Voice Assistant
voice_assistant = VoiceAssistant()

@app.route('/agent/voice', methods=['POST'])
def voice_agent():
    """Complete voice assistant endpoint"""
    try:
        if 'audio' not in request.files:
            return jsonify({'error': 'No audio file provided'}), 400
        
        audio_file = request.files['audio']
        session_id = request.form.get('session_id')
        
        # Process voice query through complete pipeline
        result = voice_assistant.process_voice_query(audio_file, session_id)
        
        if result.success:
            response_data = {
                'success': True,
                'transcription': result.transcription,
                'ai_response': result.llm_response,
                'audio_url': result.audio_url,
                'session_id': result.session_id,
                'processing_times': result.processing_times,
                'total_time': sum(result.processing_times.values()),
                'timestamp': datetime.now().isoformat()
            }
            
            return jsonify(response_data)
        else:
            return jsonify({
                'success': False,
                'error': result.error,
                'transcription': result.transcription,
                'llm_response': result.llm_response,
                'session_id': result.session_id
            }), 500
            
    except Exception as e:
        logger.error(f"Voice agent error: {str(e)}")
        return jsonify({
            'success': False,
            'error': 'Internal server error'
        }), 500

@app.route('/agent/conversation/<session_id>')
def get_conversation(session_id):
    """Get conversation history for a session"""
    if session_id in voice_assistant.conversation_sessions:
        return jsonify({
            'session_id': session_id,
            'history': voice_assistant.conversation_sessions[session_id],
            'message_count': len(voice_assistant.conversation_sessions[session_id])
        })
    else:
        return jsonify({
            'session_id': session_id,
            'history': [],
            'message_count': 0
        })
```

**Enhanced Frontend (Day 9)**:
```javascript
// Complete Voice Assistant Interface
class VoiceAssistant {
    constructor() {
        this.isRecording = false;
        this.isProcessing = false;
        this.mediaRecorder = null;
        this.audioChunks = [];
        this.sessionId = this.generateSessionId();
        this.conversationHistory = [];
    }
    
    async handleVoiceInteraction() {
        if (this.isProcessing) return;
        
        if (!this.isRecording) {
            await this.startRecording();
        } else {
            await this.stopRecording();
        }
    }
    
    async processCompleteVoiceQuery() {
        const audioBlob = new Blob(this.audioChunks, { type: 'audio/wav' });
        
        const formData = new FormData();
        formData.append('audio', audioBlob, 'voice_query.wav');
        formData.append('session_id', this.sessionId);
        
        try {
            this.updateStatus('Processing your voice query...');
            
            const response = await fetch('/agent/voice', {
                method: 'POST',
                body: formData
            });
            
            const result = await response.json();
            
            if (result.success) {
                this.displayCompleteResponse(result);
                this.updateConversationHistory(result);
            } else {
                this.displayError(result.error);
            }
            
        } catch (error) {
            this.displayError('Voice processing failed: ' + error.message);
        } finally {
            this.isProcessing = false;
            this.updateStatus('Ready for voice input');
        }
    }
    
    displayCompleteResponse(result) {
        const responseDiv = document.getElementById('voiceResponse');
        responseDiv.innerHTML = `
            <div class="voice-interaction">
                <div class="user-speech">
                    <strong>You said:</strong> "${result.transcription}"
                </div>
                <div class="ai-response">
                    <strong>AI responds:</strong> "${result.ai_response}"
                </div>
                <div class="audio-response">
                    <audio controls autoplay>
                        <source src="${result.audio_url}" type="audio/mpeg">
                    </audio>
                </div>
                <div class="processing-stats">
                    <span>STT: ${result.processing_times.stt.toFixed(2)}s</span>
                    <span>LLM: ${result.processing_times.llm.toFixed(2)}s</span>
                    <span>TTS: ${result.processing_times.tts.toFixed(2)}s</span>
                    <span>Total: ${result.total_time.toFixed(2)}s</span>
                </div>
            </div>
        `;
    }
    
    updateConversationHistory(result) {
        this.conversationHistory.push({
            user: result.transcription,
            assistant: result.ai_response,
            timestamp: result.timestamp
        });
        
        this.displayConversationHistory();
    }
}

// Initialize Complete Voice Assistant
const voiceAssistant = new VoiceAssistant();
```

**Achievements**:
- ‚úÖ Complete voice-to-voice conversation pipeline
- ‚úÖ Session-based conversation memory and context
- ‚úÖ Performance monitoring with detailed timing
- ‚úÖ Text optimization for natural speech synthesis
- ‚úÖ Robust error handling across all pipeline stages
- ‚úÖ Conversation history tracking and retrieval
- ‚úÖ Real-time processing status updates

---

## üîµ **DAY 10: Memory & Chat History**
**Goal**: Implement persistent session-based conversation memory with context management.

**üìã What I Did on Day 10:**
- Built comprehensive chat history management system
- Implemented session-based conversation storage with UUIDs
- Created context-aware conversation retrieval for LLM
- Added conversation export and import functionality
- Built conversation analytics (message count, session duration)
- Implemented conversation cleanup and archival system
- Added conversation search and filtering capabilities
- Optimized memory usage for long conversations

**Implementation**:
```python
# Day 10: Advanced Chat History and Session Management
import uuid
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional

class ChatHistoryManager:
    def __init__(self):
        self.sessions = {}
        self.session_metadata = {}
        self.max_session_age = timedelta(hours=24)
        self.max_messages_per_session = 100
    
    def create_session(self, user_id: Optional[str] = None) -> str:
        """Create a new chat session"""
        session_id = str(uuid.uuid4())
        timestamp = datetime.now()
        
        self.sessions[session_id] = []
        self.session_metadata[session_id] = {
            'created_at': timestamp.isoformat(),
            'last_activity': timestamp.isoformat(),
            'user_id': user_id,
            'message_count': 0,
            'total_tokens': 0
        }
        
        logger.info(f"Created new session: {session_id}")
        return session_id
    
    def add_message(self, session_id: str, role: str, content: str, metadata: Dict = None):
        """Add a message to the conversation history"""
        if session_id not in self.sessions:
            logger.warning(f"Session {session_id} not found, creating new session")
            self.create_session()
        
        message = {
            'id': str(uuid.uuid4()),
            'role': role,
            'content': content,
            'timestamp': datetime.now().isoformat(),
            'metadata': metadata or {}
        }
        
        self.sessions[session_id].append(message)
        
        # Update session metadata
        self.session_metadata[session_id]['last_activity'] = datetime.now().isoformat()
        self.session_metadata[session_id]['message_count'] += 1
        
        # Trim conversation if too long
        if len(self.sessions[session_id]) > self.max_messages_per_session:
            self.sessions[session_id] = self.sessions[session_id][-self.max_messages_per_session:]
    
    def get_conversation_context(self, session_id: str, max_messages: int = 10) -> str:
        """Get recent conversation context for LLM"""
        if session_id not in self.sessions:
            return ""
        
        messages = self.sessions[session_id][-max_messages:]
        context_lines = []
        
        for msg in messages:
            context_lines.append(f"{msg['role']}: {msg['content']}")
        
        return '\n'.join(context_lines)
    
    def get_session_history(self, session_id: str) -> Dict:
        """Get complete session history with metadata"""
        if session_id not in self.sessions:
            return {'error': 'Session not found'}
        
        return {
            'session_id': session_id,
            'messages': self.sessions[session_id],
            'metadata': self.session_metadata[session_id],
            'context_summary': self.generate_context_summary(session_id)
        }
    
    def generate_context_summary(self, session_id: str) -> str:
        """Generate a summary of the conversation for context"""
        if session_id not in self.sessions or not self.sessions[session_id]:
            return "No conversation history"
        
        messages = self.sessions[session_id]
        user_messages = [msg['content'] for msg in messages if msg['role'] == 'user']
        
        if len(user_messages) == 0:
            return "No user messages"
        elif len(user_messages) == 1:
            return f"User asked about: {user_messages[0][:100]}..."
        else:
            return f"Conversation topics: {', '.join(user_messages[-3:])}"
    
    def cleanup_old_sessions(self):
        """Remove sessions older than max_session_age"""
        current_time = datetime.now()
        sessions_to_remove = []
        
        for session_id, metadata in self.session_metadata.items():
            last_activity = datetime.fromisoformat(metadata['last_activity'])
            if current_time - last_activity > self.max_session_age:
                sessions_to_remove.append(session_id)
        
        for session_id in sessions_to_remove:
            del self.sessions[session_id]
            del self.session_metadata[session_id]
            logger.info(f"Cleaned up old session: {session_id}")
    
    def export_conversation(self, session_id: str) -> Optional[str]:
        """Export conversation to JSON format"""
        history = self.get_session_history(session_id)
        if 'error' in history:
            return None
        
        return json.dumps(history, indent=2)

# Initialize chat history manager
chat_manager = ChatHistoryManager()

@app.route('/api/chat/create', methods=['POST'])
def create_chat_session():
    """Create a new chat session"""
    data = request.get_json() or {}
    user_id = data.get('user_id')
    
    session_id = chat_manager.create_session(user_id)
    
    return jsonify({
        'session_id': session_id,
        'created_at': datetime.now().isoformat()
    })

@app.route('/api/chat/<session_id>/history')
def get_chat_history(session_id):
    """Get chat history for a session"""
    history = chat_manager.get_session_history(session_id)
    return jsonify(history)

@app.route('/api/chat/<session_id>/export')
def export_chat(session_id):
    """Export chat history as JSON"""
    exported_data = chat_manager.export_conversation(session_id)
    
    if not exported_data:
        return jsonify({'error': 'Session not found'}), 404
    
    return Response(
        exported_data,
        mimetype='application/json',
        headers={'Content-Disposition': f'attachment; filename=chat_{session_id}.json'}
    )

# Enhanced voice agent with chat history
@app.route('/agent/chat/<session_id>', methods=['POST'])
def agent_chat_with_history(session_id):
    """Voice agent with persistent chat history"""
    try:
        if 'audio' not in request.files:
            return jsonify({'error': 'No audio file provided'}), 400
        
        audio_file = request.files['audio']
        
        # Process voice through pipeline
        result = voice_assistant.process_voice_query(audio_file, session_id)
        
        if result.success:
            # Add to chat history
            chat_manager.add_message(session_id, 'user', result.transcription, {
                'audio_processed': True,
                'confidence': getattr(result, 'confidence', 0)
            })
            
            chat_manager.add_message(session_id, 'assistant', result.llm_response, {
                'audio_generated': True,
                'audio_url': result.audio_url
            })
            
            # Get updated conversation context
            conversation_history = chat_manager.get_conversation_context(session_id)
            
            return jsonify({
                'success': True,
                'transcription': result.transcription,
                'llm_response': result.llm_response,
                'audio_url': result.audio_url,
                'session_id': session_id,
                'conversation_history': conversation_history,
                'message_count': chat_manager.session_metadata[session_id]['message_count']
            })
        else:
            return jsonify({
                'success': False,
                'error': result.error,
                'session_id': session_id
            }), 500
            
    except Exception as e:
        logger.error(f"Chat agent error: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500
```

- ‚úÖ Message search and filtering capabilities
- ‚úÖ Conversation context summarization for LLM

---

## üîµ **DAY 11: Bulletproof Error Handling**
**Goal**: Implement comprehensive three-layer error handling system for zero-failure user experience.

**üìã What I Did on Day 11:**
- Built comprehensive three-layer error handling system
- Implemented graceful degradation for service failures
- Created emergency fallback mechanisms for critical failures
- Added client-side error recovery with retry logic
- Built detailed error logging and monitoring system
- Implemented user-friendly error messages for all scenarios
- Created emergency TTS using browser's built-in synthesis
- Added automatic service health monitoring and recovery

**Implementation**:
```python
# Day 11: Comprehensive Error Handling System
from enum import Enum
import base64
import traceback
from functools import wraps

class ErrorType(Enum):
    STT_ERROR = "stt_error"
    LLM_ERROR = "llm_error" 
    TTS_ERROR = "tts_error"
    API_ERROR = "api_error"
    NETWORK_ERROR = "network_error"
    CONFIG_ERROR = "config_error"
    VALIDATION_ERROR = "validation_error"

class ErrorHandler:
    def __init__(self):
        self.fallback_responses = {
            ErrorType.STT_ERROR: "I'm having trouble hearing you right now. Could you try speaking again?",
            ErrorType.LLM_ERROR: "I'm having trouble thinking right now. Let me try a different approach.",
            ErrorType.TTS_ERROR: "I can understand you, but I'm having trouble speaking back right now.",
            ErrorType.API_ERROR: "I'm experiencing some technical difficulties. Please try again in a moment.",
            ErrorType.NETWORK_ERROR: "I'm having trouble connecting to my services right now.",
            ErrorType.CONFIG_ERROR: "There's a configuration issue that needs attention.",
            ErrorType.VALIDATION_ERROR: "There seems to be an issue with the input provided."
        }
        
        self.emergency_responses = [
            "I apologize, but I'm experiencing technical difficulties right now.",
            "Something unexpected happened. Let me try to help you in a different way.",
            "I'm having some technical issues, but I'm still here to help.",
            "There's a temporary problem on my end. Please bear with me."
        ]
    
    def get_fallback_response(self, error_type: ErrorType, context: str = "") -> str:
        """Get appropriate fallback response for error type"""
        base_response = self.fallback_responses.get(error_type, 
                                                   "I encountered an unexpected issue.")
        
        if context:
            return f"{base_response} Context: {context}"
        return base_response
    
    def get_emergency_response(self, attempt_count: int = 0) -> str:
        """Get emergency response when all else fails"""
        index = min(attempt_count, len(self.emergency_responses) - 1)
        return self.emergency_responses[index]
    
    def generate_emergency_tts_fallback(self, text: str) -> str:
        """Generate base64 encoded text for client-side synthesis"""
        try:
            text_bytes = text.encode('utf-8')
            base64_text = base64.b64encode(text_bytes).decode('utf-8')
            return f"data:text/plain;base64,{base64_text}"
        except Exception:
            return "data:text/plain;base64,SSdtIGhhdmluZyB0cm91YmxlIHJpZ2h0IG5vdy4="  # "I'm having trouble right now."

def error_handler_decorator(fallback_error_type: ErrorType):
    """Decorator for automatic error handling"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logger.error(f"Error in {func.__name__}: {str(e)}")
                logger.error(f"Traceback: {traceback.format_exc()}")
                
                error_response = error_handler.get_fallback_response(
                    fallback_error_type, 
                    str(e)
                )
                
                return {
                    'success': False,
                    'error': error_response,
                    'error_type': fallback_error_type.value,
                    'emergency_fallback': error_handler.generate_emergency_tts_fallback(error_response)
                }
        return wrapper
    return decorator

# Initialize error handler
error_handler = ErrorHandler()

class RobustVoiceAssistant:
    def __init__(self):
        self.transcription_service = TranscriptionService()
        self.llm_service = LLMService()
        self.chat_manager = ChatHistoryManager()
        self.error_count = {}
        self.max_retries = 3
    
    def process_voice_with_fallbacks(self, audio_file, session_id: str):
        """Process voice with comprehensive error handling"""
        temp_path = None
        attempt_count = self.error_count.get(session_id, 0)
        
        try:
            # Layer 1: Primary Processing
            return self._primary_voice_processing(audio_file, session_id)
            
        except Exception as primary_error:
            logger.warning(f"Primary processing failed: {primary_error}")
            
            try:
                # Layer 2: Fallback Processing
                return self._fallback_voice_processing(audio_file, session_id, primary_error)
                
            except Exception as fallback_error:
                logger.error(f"Fallback processing failed: {fallback_error}")
                
                try:
                    # Layer 3: Emergency Processing
                    return self._emergency_voice_processing(session_id, attempt_count)
                    
                except Exception as emergency_error:
                    logger.critical(f"Emergency processing failed: {emergency_error}")
                    
                    # Final fallback - always succeeds
                    return self._final_fallback_response(session_id)
        
        finally:
            if temp_path:
                cleanup_temp_file(temp_path)
    
    def _primary_voice_processing(self, audio_file, session_id: str):
        """Primary voice processing pipeline"""
        temp_path = save_temp_file(audio_file)
        
        # STT with timeout
        transcription_result = self._safe_transcription(temp_path)
        if not transcription_result['success']:
            raise Exception(f"STT failed: {transcription_result['error']}")
        
        # LLM with context
        context = self.chat_manager.get_conversation_context(session_id)
        llm_result = self._safe_llm_processing(transcription_result['text'], context)
        if not llm_result['success']:
            raise Exception(f"LLM failed: {llm_result['error']}")
        
        # TTS with optimization
        tts_result = self._safe_tts_generation(llm_result['response'])
        if not tts_result['success']:
            raise Exception(f"TTS failed: {tts_result['error']}")
        
        # Success - reset error count
        self.error_count[session_id] = 0
        
        return {
            'success': True,
            'transcription': transcription_result['text'],
            'llm_response': llm_result['response'],
            'audio_url': tts_result['audio_url'],
            'processing_layer': 'primary'
        }
    
    def _fallback_voice_processing(self, audio_file, session_id: str, original_error):
        """Fallback processing with reduced functionality"""
        logger.info("Attempting fallback processing")
        
        # Try simpler transcription
        try:
            temp_path = save_temp_file(audio_file)
            simple_transcription = self._simple_transcription(temp_path)
            
            # Use simpler LLM prompt
            simple_response = self._simple_llm_response(simple_transcription)
            
            # Generate emergency TTS
            emergency_audio = error_handler.generate_emergency_tts_fallback(simple_response)
            
            return {
                'success': True,
                'transcription': simple_transcription,
                'llm_response': simple_response,
                'audio_url': None,
                'emergency_fallback': emergency_audio,
                'processing_layer': 'fallback',
                'warning': 'Using simplified processing due to technical issues'
            }
            
        except Exception as e:
            raise Exception(f"Fallback processing failed: {e}")
    
    def _emergency_voice_processing(self, session_id: str, attempt_count: int):
        """Emergency processing when all services fail"""
        logger.warning("Using emergency processing")
        
        emergency_message = error_handler.get_emergency_response(attempt_count)
        emergency_audio = error_handler.generate_emergency_tts_fallback(emergency_message)
        
        # Increment error count
        self.error_count[session_id] = attempt_count + 1
        
        return {
            'success': True,
            'transcription': "Unable to process audio",
            'llm_response': emergency_message,
            'audio_url': None,
            'emergency_fallback': emergency_audio,
            'processing_layer': 'emergency',
            'error': 'All services temporarily unavailable'
        }
    
    def _final_fallback_response(self, session_id: str):
        """Final fallback that always succeeds"""
        logger.critical("Using final fallback response")
        
        message = "I'm experiencing significant technical difficulties and need a moment to recover."
        emergency_audio = error_handler.generate_emergency_tts_fallback(message)
        
        return {
            'success': False,
            'transcription': None,
            'llm_response': message,
            'audio_url': None,
            'emergency_fallback': emergency_audio,
            'processing_layer': 'final_fallback',
            'error': 'Critical system error'
        }

# Initialize robust voice assistant
robust_voice_assistant = RobustVoiceAssistant()

@app.route('/api/safe-voice', methods=['POST'])
@error_handler_decorator(ErrorType.API_ERROR)
def safe_voice_processing():
    """Bulletproof voice processing endpoint"""
    if 'audio' not in request.files:
        raise ValueError("No audio file provided")
    
    audio_file = request.files['audio']
    session_id = request.form.get('session_id', str(uuid.uuid4()))
    
    result = robust_voice_assistant.process_voice_with_fallbacks(audio_file, session_id)
    
    return jsonify(result)

@app.route('/api/health/detailed')
def detailed_health_check():
    """Comprehensive health check for all services"""
    health_status = {
        'timestamp': datetime.now().isoformat(),
        'overall_status': 'healthy',
        'services': {}
    }
    
    # Check each service
    services_to_check = [
        ('stt', lambda: robust_voice_assistant.transcription_service.health_check()),
        ('llm', lambda: robust_voice_assistant.llm_service.health_check()),
        ('tts', lambda: test_murf_api()),
        ('chat', lambda: len(robust_voice_assistant.chat_manager.sessions) >= 0)
    ]
    
    for service_name, health_check_func in services_to_check:
        try:
            status = health_check_func()
            health_status['services'][service_name] = {
                'status': 'healthy' if status else 'degraded',
                'last_checked': datetime.now().isoformat()
            }
        except Exception as e:
            health_status['services'][service_name] = {
                'status': 'unhealthy',
                'error': str(e),
                'last_checked': datetime.now().isoformat()
            }
    
    # Determine overall status
    service_statuses = [s['status'] for s in health_status['services'].values()]
    if 'unhealthy' in service_statuses:
        health_status['overall_status'] = 'degraded'
    elif 'degraded' in service_statuses:
        health_status['overall_status'] = 'degraded'
    
    return jsonify(health_status)
```

**Frontend Error Handling (Day 11)**:
```javascript
// Bulletproof client-side error handling
class ErrorRecoverySystem {
    constructor() {
        this.retryCount = 0;
        this.maxRetries = 3;
        this.fallbackTTS = null;
        this.initializeFallbackTTS();
    }
    
    initializeFallbackTTS() {
        if ('speechSynthesis' in window) {
            this.fallbackTTS = window.speechSynthesis;
        }
    }
    
    async handleVoiceProcessingWithRecovery(audioBlob, sessionId) {
        try {
            // Primary attempt
            return await this.primaryVoiceProcessing(audioBlob, sessionId);
            
        } catch (error) {
            console.warn('Primary processing failed:', error);
            
            try {
                // Retry with exponential backoff
                return await this.retryWithBackoff(audioBlob, sessionId);
                
            } catch (retryError) {
                console.error('Retry failed:', retryError);
                
                // Emergency client-side fallback
                return await this.emergencyFallback(error);
            }
        }
    }
    
    async emergencyFallback(originalError) {
        const emergencyMessage = "I'm experiencing technical difficulties. Please try again in a moment.";
        
        // Use browser's built-in TTS if available
        if (this.fallbackTTS) {
            const utterance = new SpeechSynthesisUtterance(emergencyMessage);
            this.fallbackTTS.speak(utterance);
        }
        
        return {
            success: false,
            error: originalError.message,
            fallback_used: true,
            message: emergencyMessage
        };
    }
}
```

**Achievements**:
- ‚úÖ Zero-failure system with graceful degradation
- ‚úÖ Three-layer error handling (primary ‚Üí fallback ‚Üí emergency)
- ‚úÖ Emergency client-side TTS using browser synthesis
- ‚úÖ Comprehensive service health monitoring
- ‚úÖ Automatic retry logic with exponential backoff
- ‚úÖ User-friendly error messages for all scenarios
- ‚úÖ Detailed error logging and debugging information

---

## üîµ **DAY 12: Modern UI Revamp**
**Goal**: Complete interface transformation with glassmorphism design and modern UX.

**üìã What I Did on Day 12:**
- Redesigned entire interface with modern glassmorphism aesthetic
- Implemented single voice button replacing start/stop system
- Added smooth animations and transitions for better UX
- Created responsive design for mobile and desktop
- Enhanced accessibility with keyboard shortcuts and ARIA labels
- Built visual feedback system for recording states
- Optimized CSS for performance and cross-browser compatibility

**Implementation**:

```css
/* Modern glassmorphism design */
.voice-container {
    background: rgba(255, 255, 255, 0.1);
    backdrop-filter: blur(20px);
    border: 1px solid rgba(255, 255, 255, 0.2);
    border-radius: 20px;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
}

.voice-button {
    width: 200px;
    height: 200px;
    border-radius: 50%;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

.voice-button.recording {
    animation: pulse 1.5s ease-in-out infinite;
    box-shadow: 0 0 30px rgba(102, 126, 234, 0.6);
}

.voice-button.processing {
    animation: spin 2s linear infinite;
}
```

```javascript
// Single voice button logic
class VoiceAssistant {
    constructor() {
        this.isRecording = false;
        this.isProcessing = false;
        this.mediaRecorder = null;
    }
    
    async handleVoiceButtonClick() {
        if (this.isProcessing) return;
        
        if (!this.isRecording) {
            await this.startRecording();
        } else {
            await this.stopRecording();
        }
    }
    
    updateButtonState(state) {
        const button = document.getElementById('voiceButton');
        button.className = `voice-button ${state}`;
        
        const stateMessages = {
            idle: 'Click or press spacebar to start',
            recording: 'Recording... Click or press spacebar to stop',
            processing: 'Processing your request...'
        };
        
        document.getElementById('statusMessage').textContent = stateMessages[state];
    }
}
```

**Achievements**:
- ‚úÖ Modern glassmorphism UI design with frosted glass effects
- ‚úÖ Single voice button interface replacing complex controls
- ‚úÖ Smooth animations and state transitions
- ‚úÖ Responsive design for all screen sizes
- ‚úÖ Enhanced accessibility with keyboard shortcuts
- ‚úÖ Cross-browser compatibility optimization
- ‚úÖ Visual feedback for all interaction states

---

## üîµ **DAY 13: Advanced User Experience**
**Goal**: Enhance user experience with advanced interactions and feedback systems.

**üìã What I Did on Day 13:**
- Implemented advanced touch and gesture support for mobile
- Added voice command recognition for UI navigation
- Built comprehensive loading states and progress indicators
- Created contextual help system and tooltips
- Enhanced error messages with actionable suggestions
- Implemented offline detection and graceful degradation
- Added user preferences and settings persistence
- Built comprehensive keyboard navigation system

**Implementation**:
```javascript
// Day 13: Advanced UX Features
class AdvancedUXManager {
    constructor() {
        this.preferences = this.loadUserPreferences();
        this.isOnline = navigator.onLine;
        this.setupAdvancedInteractions();
        this.setupOfflineDetection();
    }
    
    setupAdvancedInteractions() {
        // Touch gesture support
        this.setupTouchGestures();
        
        // Voice commands for UI
        this.setupVoiceCommands();
        
        // Contextual help
        this.setupContextualHelp();
        
        // Keyboard navigation
        this.setupKeyboardNavigation();
    }
    
    setupTouchGestures() {
        let touchStartX, touchStartY;
        
        document.addEventListener('touchstart', (e) => {
            touchStartX = e.touches[0].clientX;
            touchStartY = e.touches[0].clientY;
        });
        
        document.addEventListener('touchend', (e) => {
            const touchEndX = e.changedTouches[0].clientX;
            const touchEndY = e.changedTouches[0].clientY;
            
            this.handleGesture(touchStartX, touchStartY, touchEndX, touchEndY);
        });
    }
    
    handleGesture(startX, startY, endX, endY) {
        const deltaX = endX - startX;
        const deltaY = endY - startY;
        
        if (Math.abs(deltaX) > Math.abs(deltaY)) {
            if (deltaX > 50) {
                this.handleSwipeRight();
            } else if (deltaX < -50) {
                this.handleSwipeLeft();
            }
        }
    }
    
    setupOfflineDetection() {
        window.addEventListener('online', () => {
            this.isOnline = true;
            this.showOfflineStatus(false);
            this.syncPendingData();
        });
        
        window.addEventListener('offline', () => {
            this.isOnline = false;
            this.showOfflineStatus(true);
            this.enableOfflineMode();
        });
    }
    
    enableOfflineMode() {
        // Switch to offline-capable features
        this.showNotification('You are offline. Some features may be limited.', 'warning');
        
        // Enable offline TTS using browser synthesis
        this.enableBrowserTTS();
    }
    
    setupKeyboardNavigation() {
        document.addEventListener('keydown', (e) => {
            // Spacebar for voice interaction
            if (e.code === 'Space' && !e.target.matches('input, textarea')) {
                e.preventDefault();
                this.toggleVoiceInteraction();
            }
            
            // Tab navigation enhancement
            if (e.key === 'Tab') {
                this.highlightFocusedElement();
            }
            
            // Escape for help/settings
            if (e.key === 'Escape') {
                this.toggleHelpOverlay();
            }
        });
    }
    
    loadUserPreferences() {
        const saved = localStorage.getItem('voiceAgentPreferences');
        return saved ? JSON.parse(saved) : {
            theme: 'auto',
            voiceSpeed: 1.0,
            autoPlay: true,
            notifications: true,
            accessibility: {
                highContrast: false,
                reduceMotion: false,
                largerText: false
            }
        };
    }
    
    saveUserPreferences() {
        localStorage.setItem('voiceAgentPreferences', JSON.stringify(this.preferences));
    }
}

// Enhanced error handling with user guidance
class UserGuidedErrorHandler {
    constructor() {
        this.errorSolutions = {
            'microphone_access_denied': {
                title: 'Microphone Access Required',
                message: 'Please allow microphone access to use voice features.',
                actions: [
                    { text: 'Try Again', action: 'requestMicrophoneAccess' },
                    { text: 'Use Text Input', action: 'switchToTextMode' }
                ]
            },
            'network_error': {
                title: 'Connection Issue',
                message: 'Having trouble connecting to our services.',
                actions: [
                    { text: 'Retry', action: 'retryLastAction' },
                    { text: 'Work Offline', action: 'enableOfflineMode' }
                ]
            },
            'audio_processing_failed': {
                title: 'Audio Processing Error',
                message: 'Could not process your audio. Try speaking more clearly.',
                actions: [
                    { text: 'Try Again', action: 'startNewRecording' },
                    { text: 'Check Microphone', action: 'testMicrophone' }
                ]
            }
        };
    }
    
    showGuidedError(errorType, context = {}) {
        const errorConfig = this.errorSolutions[errorType];
        if (!errorConfig) return;
        
        const errorModal = this.createErrorModal(errorConfig, context);
        document.body.appendChild(errorModal);
        
        // Auto-dismiss after 10 seconds if no interaction
        setTimeout(() => {
            if (errorModal.parentNode) {
                errorModal.remove();
            }
        }, 10000);
    }
    
    createErrorModal(config, context) {
        const modal = document.createElement('div');
        modal.className = 'error-modal';
        modal.innerHTML = `
            <div class="error-content">
                <h3>${config.title}</h3>
                <p>${config.message}</p>
                <div class="error-actions">
                    ${config.actions.map(action => 
                        `<button onclick="errorHandler.handleAction('${action.action}')">${action.text}</button>`
                    ).join('')}
                </div>
            </div>
        `;
        
        return modal;
    }
}

// Initialize advanced UX systems
const advancedUX = new AdvancedUXManager();
const userGuidedErrorHandler = new UserGuidedErrorHandler();
```

**Achievements**:
- ‚úÖ Advanced touch and gesture support for mobile devices
- ‚úÖ Voice command recognition for hands-free UI navigation
- ‚úÖ Comprehensive loading states and progress indicators
- ‚úÖ Contextual help system with interactive tooltips
- ‚úÖ Enhanced error messages with actionable solutions
- ‚úÖ Offline detection and graceful degradation
- ‚úÖ User preferences persistence and customization
- ‚úÖ Complete keyboard navigation accessibility

---

## üîµ **DAY 14: Code Refactoring & Clean Architecture**
**Goal**: Complete architectural overhaul with modular design, type safety, and production readiness.

**üìã What I Did on Day 14:**
- Completely refactored monolithic app.py into clean, modular architecture
- Implemented Pydantic models for type-safe API validation
- Created service layer architecture for AI integrations
- Built centralized configuration management system
- Added professional logging with structured output
- Implemented comprehensive error handling across all layers
- Created utility modules for chat history and error management
- Cleaned up project structure removing all test and debug files
Complete architectural overhaul with modular design.

**Configuration Management** (`config/settings.py`):
```python
import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    # Flask Configuration
    DEBUG = os.getenv('DEBUG', 'True').lower() == 'true'
    HOST = os.getenv('HOST', 'localhost')
    PORT = int(os.getenv('PORT', '5000'))
    
    # API Keys with validation
    ASSEMBLYAI_API_KEY = os.getenv('ASSEMBLYAI_API_KEY', '').strip("'\"")
    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', '').strip("'\"")
    MURF_API_KEY = os.getenv('MURF_API_KEY', '').strip("'\"")
    
    @classmethod
    def validate_api_keys(cls):
        return {
            'assemblyai': bool(cls.ASSEMBLYAI_API_KEY and 
                             cls.ASSEMBLYAI_API_KEY != 'your_assemblyai_api_key_here'),
            'gemini': bool(cls.GEMINI_API_KEY and 
                          cls.GEMINI_API_KEY != 'your_gemini_api_key_here'),
            'murf': bool(cls.MURF_API_KEY and 
                        cls.MURF_API_KEY != 'your_murf_api_key_here')
        }
```

**Pydantic Models** (`models/schemas.py`):
```python
from pydantic import BaseModel, Field, validator
from typing import List, Optional
from enum import Enum

class ErrorType(str, Enum):
    STT_ERROR = "stt_error"
    LLM_ERROR = "llm_error"
    TTS_ERROR = "tts_error"
    API_ERROR = "api_error"

class TTSRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=5000)
    voice_id: Optional[str] = Field(default="en-US-AriaNeural")
    speed: Optional[int] = Field(default=95, ge=50, le=200)
    pitch: Optional[int] = Field(default=45, ge=0, le=100)
    
    @validator('text')
    def text_must_not_be_empty(cls, v):
        if not v.strip():
            raise ValueError('Text cannot be empty')
        return v.strip()

class VoiceQueryResponse(BaseModel):
    success: bool
    transcription: Optional[str] = None
    llm_response: Optional[str] = None
    audio_url: Optional[str] = None
    chat_history: Optional[List[dict]] = None
    error: Optional[str] = None
    error_type: Optional[ErrorType] = None
```

**Service Layer** (`services/stt.py`):
```python
import assemblyai as aai
from config import config, logger
from models import TranscriptionResponse, ErrorType

class STTService:
    def __init__(self):
        self.api_key = config.ASSEMBLYAI_API_KEY
        self._setup_client()
    
    def _setup_client(self):
        if not self.api_key:
            logger.warning("AssemblyAI API key not configured")
            self.client = None
            return
        
        try:
            aai.settings.api_key = self.api_key
            self.client = aai
            logger.info("AssemblyAI client configured successfully")
        except Exception as e:
            logger.error(f"Failed to setup AssemblyAI client: {e}")
            self.client = None
    
    def transcribe_audio(self, audio_file):
        if not self.is_available():
            return TranscriptionResponse(
                success=False,
                error="STT service not available",
                error_type=ErrorType.CONFIG_ERROR
            )
        
        try:
            temp_file_path = self._save_temp_file(audio_file)
            transcript = self.client.Transcriber().transcribe(temp_file_path)
            
            return TranscriptionResponse(
                success=True,
                transcription=transcript.text,
                confidence=getattr(transcript, 'confidence', None)
            )
        except Exception as e:
            logger.error(f"STT transcription failed: {e}")
            return TranscriptionResponse(
                success=False,
                error=str(e),
                error_type=ErrorType.STT_ERROR
            )
```

**Main Application** (`app.py`):
```python
from flask import Flask, request, jsonify
from pydantic import ValidationError

from config import config, logger
from models import TTSRequest, VoiceQueryResponse, ErrorType
from services import stt_service, llm_service, tts_service
from utils import chat_manager, error_handler

class VoiceAgentApp:
    def __init__(self):
        self.app = Flask(__name__)
        self._configure_app()
        self._register_routes()
    
    def _register_routes(self):
        @self.app.route('/api/health')
        def health_check():
            services_status = {
                'stt': stt_service.is_available(),
                'llm': llm_service.is_available(),
                'tts': tts_service.is_available()
            }
            
            return jsonify({
                "status": "healthy" if all(services_status.values()) else "degraded",
                "services": services_status,
                "version": "1.0.0",
                "day": 14
            })
        
        @self.app.route('/llm/query', methods=['POST'])
        def voice_query():
            try:
                if 'audio' not in request.files:
                    return jsonify({"error": "No audio file provided"}), 400
                
                audio_file = request.files['audio']
                session_id = request.form.get('session_id', str(uuid.uuid4()))
                
                response = self._process_voice_pipeline(audio_file, session_id)
                return jsonify(response.model_dump())
                
            except Exception as e:
                logger.error(f"Voice query error: {e}")
                return jsonify({"error": str(e)}), 500

# Create and run application
app_instance = VoiceAgentApp()
app = app_instance.app

if __name__ == '__main__':
    app_instance.run()
```

**Achievements**: Clean architecture, type safety, service separation, comprehensive testing.

---

## üöÄ API Documentation

### **Health Check**
```bash
GET /api/health
```
**Response:**
```json
{
  "status": "healthy",
  "services": {
    "stt": true,
    "llm": true,
    "tts": true
  },
  "version": "1.0.0",
  "day": 14
}
```

### **Voice Query (Complete Pipeline)**
```bash
POST /llm/query
Content-Type: multipart/form-data

audio: <audio_file>
session_id: <optional_session_id>
```
**Response:**
```json
{
  "success": true,
  "transcription": "Hello, how are you today?",
  "llm_response": "I'm doing well, thank you for asking! How can I help you?",
  "audio_url": "https://cdn.murf.ai/audio/xyz.mp3",
  "chat_history": [
    {"role": "user", "content": "Hello, how are you today?"},
    {"role": "assistant", "content": "I'm doing well, thank you!"}
  ],
  "session_id": "uuid-string",
  "message_count": 2
}
```

### **Text-to-Speech**
```bash
POST /api/tts
Content-Type: application/json

{
  "text": "Hello world",
  "voice_id": "en-US-AriaNeural",
  "speed": 95,
  "pitch": 45
}
```

### **LLM Text Query**
```bash
POST /llm/query
Content-Type: application/json

{
  "text": "What is artificial intelligence?"
}
```

---

## üß™ Testing

### **Run Test Suite**
```bash
# Comprehensive test suite
python test_day14_refactor.py

# API endpoint tests
python test_day14_api.py
```

### **Manual Testing Commands**
```bash
# Health check
curl http://localhost:5000/api/health

# Test LLM endpoint
curl -X POST http://localhost:5000/llm/query \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello AI!"}'

# Test TTS endpoint
curl -X POST http://localhost:5000/api/tts \
  -H "Content-Type: application/json" \
  -d '{"text": "This is a test"}'
```

---

## üõ°Ô∏è Error Handling

The application implements a comprehensive three-layer error handling system:

### **Layer 1: Service-Level Errors**
- API timeouts and failures
- Invalid responses
- Service unavailability

### **Layer 2: Application-Level Errors**
- Request validation errors
- Processing failures
- Resource constraints

### **Layer 3: Client-Level Fallbacks**
- Emergency TTS using browser synthesis
- User-friendly error messages
- Retry mechanisms

### **Error Response Format**
```json
{
  "success": false,
  "error": "User-friendly error message",
  "error_type": "api_error",
  "emergency_fallback": "data:text/plain;base64,....." 
}
```

---

## üîß Configuration

### **Environment Variables**
```bash
# Required API Keys
ASSEMBLYAI_API_KEY=your_key_here
GEMINI_API_KEY=your_key_here  
MURF_API_KEY=your_key_here
MURF_API_URL=your_url_here

# Optional Application Settings
DEBUG=True
HOST=localhost
PORT=5000
LOG_LEVEL=INFO
UPLOAD_FOLDER=uploads
MAX_CONTENT_LENGTH=16777216
```

### **Service Configuration**
```python
# Default voice settings
DEFAULT_VOICE_ID=en-US-AriaNeural
DEFAULT_SPEECH_SPEED=95
DEFAULT_SPEECH_PITCH=45

# Timeout settings
STT_TIMEOUT=30
LLM_TIMEOUT=30
TTS_TIMEOUT=60
```

---

## üöÄ Deployment

### **Development**
```bash
python app.py
```

### **Production (with Gunicorn)**
```bash
pip install gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 app:app
```

### **Docker Deployment**
```dockerfile
FROM python:3.12-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 5000

CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "app:app"]
```

---

## üéØ Key Achievements

### **Technical Excellence**
- ‚úÖ **Clean Architecture**: Modular, maintainable codebase
- ‚úÖ **Type Safety**: Pydantic models with validation
- ‚úÖ **Error Resilience**: Three-layer fallback system
- ‚úÖ **Production Ready**: Logging, monitoring, health checks
- ‚úÖ **Test Coverage**: Comprehensive automated testing

### **User Experience**
- ‚úÖ **Modern UI**: Glassmorphism design with animations
- ‚úÖ **Accessibility**: Keyboard shortcuts, ARIA labels
- ‚úÖ **Responsive**: Mobile-first design approach
- ‚úÖ **Intuitive**: Single button interface with clear feedback
- ‚úÖ **Reliable**: Zero-failure user experience

### **AI Integration**
- ‚úÖ **Multi-Modal**: Voice, text, and audio processing
- ‚úÖ **Conversational**: Memory and context awareness
- ‚úÖ **Intelligent**: Natural language understanding
- ‚úÖ **Scalable**: Service-oriented architecture
- ‚úÖ **Performant**: Optimized processing pipeline

---

## üîÆ Future Roadmap (Days 15-30)

### **Advanced Features**
- Real-time streaming conversations
- Multi-language support
- Voice customization and cloning
- Emotion detection and response
- Advanced conversation management

### **Technical Enhancements**
- WebSocket real-time communication
- Redis for session storage
- Docker containerization
- CI/CD pipeline setup
- Performance monitoring and analytics

### **Enterprise Features**
- User authentication and authorization
- API rate limiting and quotas
- Advanced security measures
- Multi-tenant architecture
- Scalable deployment options

---

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## üôè Acknowledgments

- **Google Gemini AI** for advanced language processing
- **Murf AI** for high-quality text-to-speech synthesis
- **AssemblyAI** for accurate speech-to-text transcription
- **Flask** for the robust web framework
- **Pydantic** for data validation and type safety

---

## üìû Support

For questions, issues, or contributions:
- Create an issue in the GitHub repository
- Check the API documentation above
- Review the test files for usage examples

**Built with ‚ù§Ô∏è during the 30 Days of AI Voice Agents Challenge**
